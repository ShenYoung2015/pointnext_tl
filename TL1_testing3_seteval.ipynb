{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PointNeXt(\n",
       "  (mlp): Conv1d(6, 32, kernel_size=(1,), stride=(1,))\n",
       "  (stage): ModuleList(\n",
       "    (0): Stage(\n",
       "      (sa): SetAbstraction(\n",
       "        (mlp): Sequential(\n",
       "          (0): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (irm): Sequential(\n",
       "        (0): InvResMLP(\n",
       "          (la): LocalAggregation(\n",
       "            (mlp): Sequential(\n",
       "              (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (pw_conv): Sequential(\n",
       "            (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Stage(\n",
       "      (sa): SetAbstraction(\n",
       "        (mlp): Sequential(\n",
       "          (0): Conv2d(67, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (irm): Sequential(\n",
       "        (0): InvResMLP(\n",
       "          (la): LocalAggregation(\n",
       "            (mlp): Sequential(\n",
       "              (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (pw_conv): Sequential(\n",
       "            (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): InvResMLP(\n",
       "          (la): LocalAggregation(\n",
       "            (mlp): Sequential(\n",
       "              (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (pw_conv): Sequential(\n",
       "            (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Stage(\n",
       "      (sa): SetAbstraction(\n",
       "        (mlp): Sequential(\n",
       "          (0): Conv2d(131, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (irm): Sequential(\n",
       "        (0): InvResMLP(\n",
       "          (la): LocalAggregation(\n",
       "            (mlp): Sequential(\n",
       "              (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (pw_conv): Sequential(\n",
       "            (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Stage(\n",
       "      (sa): SetAbstraction(\n",
       "        (mlp): Sequential(\n",
       "          (0): Conv2d(259, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (irm): Sequential(\n",
       "        (0): InvResMLP(\n",
       "          (la): LocalAggregation(\n",
       "            (mlp): Sequential(\n",
       "              (0): Conv2d(515, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (pw_conv): Sequential(\n",
       "            (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): FeaturePropagation(\n",
       "      (mlp_modules): Sequential(\n",
       "        (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): FeaturePropagation(\n",
       "      (mlp_modules): Sequential(\n",
       "        (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): FeaturePropagation(\n",
       "      (mlp_modules): Sequential(\n",
       "        (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): FeaturePropagation(\n",
       "      (mlp_modules): Sequential(\n",
       "        (0): Conv1d(96, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): RegHead(\n",
       "    (mlp_modules): Sequential(\n",
       "      (0): Conv1d(32, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Model import PointNeXt\n",
    "# from dataset.FFDshape import FFDshape_ptp\n",
    "from Loss import reg_loss\n",
    "from Transforms import PCDPretreatment, get_data_augment\n",
    "import numpy as np\n",
    "from numpy.core.umath import isnan\n",
    "from Parameters import *\n",
    "sys.path.insert(1, os.path.dirname(os.path.abspath(__name__)))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_cfg = MODEL_CONFIG['basic_c']\n",
    "max_input = model_cfg['max_input']\n",
    "normal = model_cfg['normal']\n",
    "model = PointNeXt(model_cfg).to(device=device)\n",
    "\n",
    "checkpoint_name = 'PointNeXt_shapesffd3_epoch1000.pth'\n",
    "checkpoint_dir = 'result_train//PointNeXt_model=basic_c_ds=shapesffd3_aug=basic_lr=0.001_wd=0.0001_bs=16_AdamW_cosine//'\n",
    "checkpoint_file = checkpoint_dir + checkpoint_name\n",
    "\n",
    "# checkpoint_file = 'result_train\\PointNeXt_model=basic_c_ds=waverider_aug=basic_lr=1e-07_wd=0.0001_bs=8_AdamW_cosine\\PointNeXt_waverider_epoch30.pth'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFDshape_eval(Dataset):\n",
    "    def __init__(self, root, transforms=None, split='test', npoints=1024, augment=False, dp=False, normalize=False):\n",
    "        assert(split == 'train' or split == 'test')\n",
    "        self.npoints = npoints\n",
    "        self.transforms = transforms\n",
    "        self.train_files_list = []\n",
    "        self.test_files_list = []\n",
    "        if split == 'train':\n",
    "            self.training = True\n",
    "        elif split == 'test':\n",
    "            self.training = False\n",
    "        \n",
    "        name_list = os.listdir(os.path.join(root,'pc'))\n",
    "        for i in range(len(name_list)):\n",
    "            name_list[i] = os.path.splitext(name_list[i])[0]\n",
    "        \n",
    "        test_files_list = self.read_list_file(name_list, root)\n",
    "        self.test_files_list = test_files_list\n",
    "\n",
    "        # self.train_files_list = train_files_list # train_files_list\n",
    "\n",
    "        self.caches = {}\n",
    "        print(\n",
    "            f'Training {len(self.train_files_list)} shapes. Testing {len(self.test_files_list)} shapes '\n",
    "        )\n",
    "\n",
    "    def read_list_file(self, name_list, root):\n",
    "        # base = os.path.dirname(file_path)\n",
    "        files_list = []\n",
    "        for shape_name in name_list:\n",
    "            cur = os.path.join(root, 'pc', '{}.txt'.format(shape_name))\n",
    "            files_list.append(cur)\n",
    "        return files_list\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index in self.caches:\n",
    "            return self.caches[index]\n",
    "        file = self.pcd[index]\n",
    "        pc = np.loadtxt(file, delimiter=',').astype(np.float32)\n",
    "        xyz_points = pc[:, :6]\n",
    "        gts = pc[:, 6]\n",
    "        s_mesh = pc[:, -1]\n",
    "\n",
    "        # resample\n",
    "        # choice = np.random.choice(len(xyz_points), self.npoints, replace=True)\n",
    "        # xyz_points = xyz_points[choice, :]\n",
    "        # gts = gts[choice]\n",
    "\n",
    "        xyz_points = torch.from_numpy(xyz_points).float()\n",
    "        gts = torch.from_numpy(gts).float()\n",
    "        if self.transforms is not None:\n",
    "            xyz_points, gts = self.transforms(xyz_points, gts)\n",
    "        else:\n",
    "            xyz_points = xyz_points.T\n",
    "            \n",
    "        return xyz_points, gts, s_mesh\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pcd)\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        self.pcd = self.train_files_list\n",
    "        if self.transforms is not None:\n",
    "            self.transforms.set_mode('train')\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        self.pcd = self.test_files_list\n",
    "        if self.transforms is not None:\n",
    "            self.transforms.set_mode('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read *.stl\n",
    "def stl_read(shape_file):\n",
    "    import open3d as o3d\n",
    "    my_mesh = o3d.io.read_triangle_mesh(shape_file)\n",
    "    Points = np.asarray(my_mesh.vertices)\n",
    "    Connectivity = np.asarray(my_mesh.triangles)\n",
    "\n",
    "    # rescale to 25 m in X-axis\n",
    "    rate = 25/((Points[:,0]).max()-(Points[:,0]).min())\n",
    "    Points = Points * rate\n",
    "    Points[:,0] = Points[:,0] - Points[:,0].mean()\n",
    "    \n",
    "    return Points, Connectivity, rate\n",
    "\n",
    "# more feature of pc\n",
    "def ex_feature(Points, Connectivity):\n",
    "    normals = np.zeros(np.shape(Connectivity))\n",
    "    for i in range(len(Connectivity)):\n",
    "        temp1 = np.cross(Points[Connectivity[i,2],:] - Points[Connectivity[i,0],:],\n",
    "            Points[Connectivity[i,1],:] - Points[Connectivity[i,0],:]); # mesh outer direction\n",
    "        normals[i,:] = temp1/np.linalg.norm(temp1,ord=2)\n",
    "    normals = -normals\n",
    "    \n",
    "    # tri face points\n",
    "    Points_tri = np.zeros(np.shape(Connectivity))\n",
    "    for i in range(len(Connectivity)):\n",
    "        Points_tri[i,:] = np.mean(Points[Connectivity[i,:],:],axis=0)\n",
    "\n",
    "    # cal tri-area\n",
    "    tri_point = np.zeros((np.size(Connectivity,0),3,3))\n",
    "    side_len = np.zeros((np.size(Connectivity,0),3))\n",
    "    for i in range(3):\n",
    "        tri_point[:,:,i] = Points[Connectivity[:,i],:]\n",
    "\n",
    "    for i in range(3):\n",
    "        tmp = np.array([i,i+1]).astype(int)\n",
    "        tmp[tmp>=3] = 0\n",
    "        side_len[:,i] = np.sqrt(np.sum(\n",
    "            (tri_point[:,:,tmp[0]] - tri_point[:,:,tmp[1]])**2\n",
    "            ,1))\n",
    "    side_p = np.sum(side_len,1)/2\n",
    "    tri_area = np.sqrt(side_p*\n",
    "        (side_p-side_len[:,0]) *\n",
    "        (side_p-side_len[:,1]) *\n",
    "        (side_p-side_len[:,2]))\n",
    "    tri_area[isnan(tri_area)]=0\n",
    "    return normals, Points_tri, tri_area\n",
    "\n",
    "# rotate Y-axis\n",
    "def rotate_pc(pc, rotation_angle):\n",
    "    cosval = np.cos(np.deg2rad(rotation_angle))\n",
    "    sinval = np.sin(np.deg2rad(rotation_angle))\n",
    "    rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                [0, 1, 0],\n",
    "                                [-sinval, 0, cosval]])\n",
    "    rotated_pc_xyz = np.dot(pc[:,:3], rotation_matrix)\n",
    "    if pc.shape[-1] > 3:\n",
    "        rotated_pc_normal = np.dot(pc[:,3:], rotation_matrix)\n",
    "        rotated_pc = np.concatenate((rotated_pc_xyz, rotated_pc_normal), axis=1)\n",
    "    else:\n",
    "        rotated_pc = rotated_pc_xyz\n",
    "    return torch.tensor(rotated_pc)\n",
    "\n",
    "def uniform_pc(pcd):\n",
    "    scale_rate = np.zeros(pcd.shape[0])\n",
    "    shift = np.zeros((pcd.shape[0],3))\n",
    "    pcd_out = torch.zeros(pcd.shape)\n",
    "    for i in range(pcd.shape[0]):\n",
    "        pcd_tmp = pcd[i,:,:].squeeze(0).T\n",
    "        # 坐标归一化\n",
    "        pcd_xyz = pcd_tmp[:, :3]\n",
    "        pcd_xyz = pcd_xyz - pcd_xyz.mean(dim=0, keepdim=True)\n",
    "        dis = torch.norm(pcd_xyz, dim=1)\n",
    "        max_dis = dis.max()\n",
    "        pcd_xyz /= max_dis\n",
    "        scale_rate[i] = max_dis\n",
    "        shift[i,:] = pcd_xyz.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # 法线\n",
    "        if pcd_tmp.shape[1]>3:\n",
    "            pcd_tmp[:, :3] = pcd_xyz\n",
    "        else:\n",
    "            pcd_tmp = pcd_xyz\n",
    "        pcd_out[i,:,:] = pcd_tmp.T.unsqueeze(0)\n",
    "\n",
    "    return pcd_out, scale_rate, shift\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "    跟据采样点索引获取其原始点云xyz坐标等信息\n",
    "    :param points: <torch.Tensor> (B, N, 3+) 原始点云\n",
    "    :param idx: <torch.Tensor> (B, S)/(B, S, G) 采样点索引，S为采样点数量，G为每个采样点grouping的点数\n",
    "    :return: <torch.Tensor> (B, S, 3+)/(B, S, G, 3+) 获取了原始点云信息的采样点\n",
    "    \"\"\"\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long, device=points.device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    最远点采样\n",
    "    随机选择一个初始点作为采样点，循环的将与当前采样点距离最远的点当作下一个采样点，直至满足采样点的数量需求\n",
    "    :param xyz: <torch.Tensor> (B, N, 3+) 原始点云\n",
    "    :param npoint: <int> 采样点数量\n",
    "    :return: <torch.Tensor> (B, npoint) 采样点索引\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    npoint = min(npoint, N)\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10  # 每个点与最近采样点的最小距离\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)  # 随机选取初始点\n",
    "\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, -1)  # [bs, 1, coor_dim]\n",
    "        dist = torch.nn.functional.pairwise_distance(xyz, centroid)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_interpolate(xyz1, xyz2, points2):\n",
    "    '''\n",
    "\n",
    "    :param xyz1: shape=(B, N1, 3)\n",
    "    :param xyz2: shape=(B, N2, 3)\n",
    "    :param points2: shape=(B, N2, C2)\n",
    "    :return: interpolated_points: shape=(B, N1, C2)\n",
    "    '''\n",
    "    interp_num = 3\n",
    "    _, _, C2 = points2.shape\n",
    "    dists, inds = three_nn(xyz1, xyz2, interp_num)\n",
    "    inversed_dists = 1.0 / (dists + 1e-8)\n",
    "    weight = inversed_dists / torch.sum(inversed_dists, dim=-1, keepdim=True) # shape=(B, N1, 3)\n",
    "    weight = torch.unsqueeze(weight, -1).repeat(1, 1, 1, C2)\n",
    "    interpolated_points = gather_points(points2, inds)  # shape=(B, N1, 3, C2)\n",
    "    interpolated_points = torch.sum(weight * interpolated_points, dim=2)\n",
    "    return interpolated_points\n",
    "\n",
    "def three_nn(xyz1, xyz2, interp_num=3):\n",
    "    '''\n",
    "\n",
    "    :param xyz1: shape=(B, N1, 3)\n",
    "    :param xyz2: shape=(B, N2, 3)\n",
    "    :return: dists: shape=(B, N1, 3), inds: shape=(B, N1, 3)\n",
    "    '''\n",
    "    dists = get_dists(xyz1, xyz2)\n",
    "    dists, inds = torch.sort(dists, dim=-1)\n",
    "    dists, inds = dists[:, :, :interp_num], inds[:, :, :interp_num]\n",
    "    return dists, inds\n",
    "\n",
    "def get_dists(points1, points2):\n",
    "    '''\n",
    "    Calculate dists between two group points\n",
    "    :param cur_point: shape=(B, M, C)\n",
    "    :param points: shape=(B, N, C)\n",
    "    :return: \n",
    "    '''\n",
    "    B, M, C = points1.shape\n",
    "    _, N, _ = points2.shape\n",
    "    dists = torch.sum(torch.pow(points1, 2), dim=-1).view(B, M, 1) + \\\n",
    "            torch.sum(torch.pow(points2, 2), dim=-1).view(B, 1, N)\n",
    "    dists -= 2 * torch.matmul(points1, points2.permute(0, 2, 1))\n",
    "    dists = torch.where(dists < 0, torch.ones_like(dists) * 1e-7, dists) # Very Important for dist = 0.\n",
    "    return torch.sqrt(dists).float()\n",
    "\n",
    "def gather_points(points, inds):\n",
    "    '''\n",
    "\n",
    "    :param points: shape=(B, N, C)\n",
    "    :param inds: shape=(B, M) or shape=(B, M, K)\n",
    "    :return: sampling points: shape=(B, M, C) or shape=(B, M, K, C)\n",
    "    '''\n",
    "    device = points.device\n",
    "    B, N, C = points.shape\n",
    "    inds_shape = list(inds.shape)\n",
    "    inds_shape[1:] = [1] * len(inds_shape[1:])\n",
    "    repeat_shape = list(inds.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batchlists = torch.arange(0, B, dtype=torch.long).to(device).reshape(inds_shape).repeat(repeat_shape)\n",
    "    return points[batchlists, inds, :]\n",
    "def all_nn(xyz1, xyz2):\n",
    "    '''\n",
    "\n",
    "    :param xyz1: shape=(B, N1, 3)\n",
    "    :param xyz2: shape=(B, N2, 3)\n",
    "    :return: dists: shape=(B, N1, N2), inds: shape=(B, N1, N2)\n",
    "    '''\n",
    "    dists = get_dists(xyz1, xyz2)\n",
    "    dists, inds = torch.sort(dists, dim=-1)\n",
    "    return dists, inds\n",
    "\n",
    "def three_interpolate_normals(xyz1, xyz2, points2, normals1, normals2):\n",
    "    '''\n",
    "\n",
    "    :param xyz1: shape=(B, N1, 3)\n",
    "    :param xyz2: shape=(B, N2, 3)\n",
    "    :param points2: shape=(B, N2, C2)\n",
    "    :return: interpolated_points: shape=(B, N1, C2)\n",
    "    '''\n",
    "    interp_num = 12\n",
    "    _, _, C2 = points2.shape\n",
    "    dists_normals = torch.matmul(normals1, normals2.permute(0, 2, 1))\n",
    "    dists, inds = all_nn(xyz1, xyz2)\n",
    "    \n",
    "    # !!!矩阵化索引有问题\n",
    "    # interp_num_tmp = interp_num\n",
    "    # while not torch.all(torch.any((dists_normals[:,:,inds[0,i,:interp_num_tmp]]>0),dim=2)):\n",
    "    #     interp_num_tmp = interp_num_tmp + 1\n",
    "    #     print(interp_num_tmp)\n",
    "    # for i in range(dists_normals.shape[1]):\n",
    "    #     dists_normals[:,i,:interp_num_tmp] = dists_normals[:,i,inds[0,i,:interp_num_tmp]]\n",
    "    \n",
    "    # 粗暴方式\n",
    "    interp_num_tmp = interp_num\n",
    "    dists_normals_tmp = dists_normals[:,:,:64]\n",
    "    while True:\n",
    "        for i in range(dists_normals.shape[1]):\n",
    "            dists_normals_tmp[:,i,:interp_num_tmp] = dists_normals[:,i,inds[0,i,:interp_num_tmp]]\n",
    "        if torch.all(torch.any((dists_normals_tmp[:,:,:interp_num_tmp]>0),dim=2)):\n",
    "            dists_normals = dists_normals_tmp\n",
    "            break\n",
    "        else:\n",
    "            interp_num_tmp = interp_num_tmp + 2\n",
    "            print(interp_num_tmp)\n",
    "    \n",
    "    dists_normals = dists_normals[:,:,:interp_num_tmp]\n",
    "    dists_normals = (dists_normals+1e-7)\n",
    "    dists_normals[dists_normals<=0] = 0\n",
    "    inversed_dists = 1.0 / (dists[:,:,:interp_num_tmp] + 1e-8) *dists_normals\n",
    "    weight = inversed_dists / torch.sum(inversed_dists, dim=-1, keepdim=True) # shape=(B, N1, 3)\n",
    "    weight = torch.unsqueeze(weight, -1).repeat(1, 1, 1, C2)\n",
    "    interpolated_points = gather_points(points2, inds[:,:,:interp_num_tmp])  # shape=(B, N1, 3, C2)\n",
    "    interpolated_points = torch.sum(weight * interpolated_points, dim=2)\n",
    "    return interpolated_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_base = 'D:\\MyCode\\PCdeep_TL\\ShapeGenerator\\shape_set\\waverider\\\\testing_N200_D60_322'\n",
    "shape_base = '..\\ShapeGenerator\\shape_set\\waverider\\\\testing_N200_D60_322'\n",
    "param_path = os.path.join(shape_base,'paramList.csv')\n",
    "param_list = np.loadtxt(param_path, delimiter=',')\n",
    "aoa_list = param_list[:,-1]\n",
    "# # Tri-reading\n",
    "# shape_file = shape_base + 'shape_001.stl'\n",
    "# if shape_file[-3:] == 'stl' or 'ply':\n",
    "#     Points_init, Connectivity, rate = stl_read(shape_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0 shapes. Testing 200 shapes \n"
     ]
    }
   ],
   "source": [
    "data_root = '..\\shapesffd4'\n",
    "pc_root = data_root + '\\waverider\\\\testing_N200_D60_322'\n",
    "# transforms = PCDPretreatment(num=1024, down_sample='random', normal=model_cfg['normal'])\n",
    "transforms = None\n",
    "dataset = FFDshape_eval(root=pc_root,split='test',transforms=transforms)\n",
    "dataset.eval()\n",
    "eval_dataloader = DataLoader(dataset=dataset,\n",
    "                                batch_size=1,\n",
    "                                num_workers=0,\n",
    "                                pin_memory=False,#True\n",
    "                                drop_last=False,\n",
    "                                shuffle=False)\n",
    "criterion = reg_loss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn import metrics\n",
    "torch.manual_seed(0)\n",
    "# seed 0\n",
    "# CP Direct, MAE: 0.0323; RMSE: 0.0698; R2: 0.8609 \n",
    "# CP Interp, MAE: 0.0390; RMSE: 0.0810; R2: 0.7945 \n",
    "# cds, MAE: 7.7161; RMSE: 11.3791; R2: 0.9939; relative l2: 0.0524\n",
    "# seed 1234\n",
    "# CP Direct, MAE: 0.0318; RMSE: 0.0676; R2: 0.8629 \n",
    "# CP Interp, MAE: 0.0387; RMSE: 0.0803; R2: 0.7903 \n",
    "# cds, MAE: 7.9880; RMSE: 12.1252; R2: 0.9931; relative l2: 0.0558\n",
    "\n",
    "disp_loss = False\n",
    "\n",
    "mse_list = np.zeros(len(eval_dataloader))\n",
    "mae_list = np.zeros(len(eval_dataloader))\n",
    "rmse_list = np.zeros(len(eval_dataloader))\n",
    "r2_score_list = np.zeros(len(eval_dataloader))\n",
    "mae_all_list = np.zeros(len(eval_dataloader))\n",
    "rmse_all_list = np.zeros(len(eval_dataloader))\n",
    "r2_score_all_list = np.zeros(len(eval_dataloader))\n",
    "cds_pred = np.zeros(len(eval_dataloader))\n",
    "cds_gt = np.zeros(len(eval_dataloader))\n",
    "\n",
    "\n",
    "num_points = 1024\n",
    "for count, data in enumerate(eval_dataloader):\n",
    "    # size pcd_all: [B,C,N] gts: [B,N]\n",
    "    pcd_all, gts_all, s_tri = data\n",
    "    # pcd_rotated = rotate_pc(pcd_all.squeeze(0).T, -aoa_list[count])\n",
    "    # pcd_all = pcd_rotated.clone().detach().T.unsqueeze(0).float()\n",
    "    pcd_uniform = copy.deepcopy(pcd_all)\n",
    "    pcd_uniform, scale_rate, shift = uniform_pc(pcd_uniform)\n",
    "    choice_idx = torch.randperm(pcd_uniform.shape[2])[:num_points] # random sampling\n",
    "#     choice_idx = farthest_point_sample(pcd_all.permute(0,2,1)[:,:,:3], num_points).squeeze(0) # fps\n",
    "    pcd = pcd_uniform[:,:,choice_idx]\n",
    "    pcd = pcd.to(device, non_blocking=True)\n",
    "    gts = gts_all[:,choice_idx] \n",
    "    # model pred\n",
    "    with torch.no_grad():\n",
    "        pred = model(pcd)\n",
    "        pred = torch.squeeze(pred,1)# Size([1, 1024])\n",
    "        if disp_loss:\n",
    "            print(criterion(pred, gts.to(device)))\n",
    "    # re scale\n",
    "    pred, gts, pcd = pred.to('cpu'), np.array(gts.to('cpu')), pcd.to('cpu')\n",
    "    pcd[0,:3,:] = pcd[0,:3,:] * scale_rate\n",
    "    pcd[0,:3,:] += shift.T\n",
    "    # to np\n",
    "    pcd_np = np.array(pcd.squeeze(0).T[:,:3])\n",
    "    pred_np = np.array(pred.squeeze(0))\n",
    "    pcd_all_np = np.array(pcd_all.squeeze(0).T[:,:3])\n",
    "\n",
    "    # interp\n",
    "    pred_all = three_interpolate_normals(pcd_all[:,:3,:].permute(0,2,1), \\\n",
    "                            pcd[:,:3,:].permute(0,2,1).to(torch.float32),\\\n",
    "                            pred.unsqueeze(2),\\\n",
    "                            pcd_all[:,3:6,:].permute(0,2,1),\\\n",
    "                            pcd[:,3:6,:].permute(0,2,1).to(torch.float32))\n",
    "    # to 1 dim\n",
    "    pred_1d = np.array(pred[0])\n",
    "    gts_1d = np.array(gts[0])\n",
    "    pred_1d_all = np.array(pred_all[0,:,0])\n",
    "    gts_1d_all = np.array(gts_all[0,:])\n",
    "    \n",
    "    # cp metrics per shape\n",
    "    mse = metrics.mean_squared_error(pred_1d, gts_1d)\n",
    "    mae = metrics.mean_absolute_error(pred_1d, gts_1d)\n",
    "    rmse = metrics.mean_squared_error(pred_1d, gts_1d)**0.5\n",
    "    r2_score = metrics.r2_score(pred_1d, gts_1d)\n",
    "    print('Direct-%d, MAE: %.4f; RMSE: %.4f; R2: %.4f; MSE: %.6f' %(count, mae, rmse, r2_score, mse))\n",
    "\n",
    "    mae_all = metrics.mean_absolute_error(pred_1d_all, gts_1d_all)\n",
    "    rmse_all = metrics.mean_squared_error(pred_1d_all, gts_1d_all)**0.5\n",
    "    r2_score_all = metrics.r2_score(pred_1d_all, gts_1d_all)\n",
    "    print('Interp-%d, MAE: %.4f; RMSE: %.4f; R2: %.4f ' %(count, mae_all, rmse_all, r2_score_all))\n",
    "\n",
    "    # cp Metrics\n",
    "    mse_list[count] = mse\n",
    "    mae_list[count] = mae\n",
    "    rmse_list[count] = rmse\n",
    "    r2_score_list[count] = r2_score\n",
    "    mae_all_list[count] = mae_all\n",
    "    rmse_all_list[count] = rmse_all\n",
    "    r2_score_all_list[count] = r2_score_all\n",
    "    # cal forces\n",
    "    normals_all_np = np.array(pcd_all.squeeze(0).T[:,3:])\n",
    "    s_tri_np = np.array(s_tri)[0]\n",
    "    force_pred = np.sum(normals_all_np*\\\n",
    "            np.repeat(np.expand_dims(pred_1d_all,axis=1),3,axis=1)*\\\n",
    "            np.repeat(np.expand_dims(s_tri_np,axis=1),3,axis=1), axis=0)\n",
    "    force_gt = np.sum(normals_all_np*\\\n",
    "            np.repeat(np.expand_dims(gts_1d_all,axis=1),3,axis=1)*\\\n",
    "            np.repeat(np.expand_dims(s_tri_np,axis=1),3,axis=1), axis=0)\n",
    "    cds_pred[count] = force_pred[0]\n",
    "    cds_gt[count] = force_gt[0]\n",
    "\n",
    "# metrics summary\n",
    "# cp of direct set\n",
    "mse_set = np.mean(mse_list)\n",
    "mae_set = np.mean(mae_list)\n",
    "rmse_set = np.mean(rmse_list)\n",
    "r2_score_set = np.mean(r2_score_list)\n",
    "print('CP Direct, MAE: %.4f; RMSE: %.4f; R2: %.4f; MSE: %.6f' %(mae_set, rmse_set, r2_score_set, mse_set))\n",
    "# cp of interp set\n",
    "mae_all_set = np.mean(mae_all_list)\n",
    "rmse_all_set = np.mean(rmse_all_list)\n",
    "r2_score_all_set = np.mean(r2_score_all_list)\n",
    "print('CP Interp, MAE: %.4f; RMSE: %.4f; R2: %.4f ' %(mae_all_set, rmse_all_set, r2_score_all_set))\n",
    "# cds\n",
    "cds_mae = metrics.mean_absolute_error(cds_pred, cds_gt)\n",
    "cds_rmse = metrics.mean_squared_error(cds_pred, cds_gt)**0.5\n",
    "cds_r2_score = metrics.r2_score(cds_pred, cds_gt)\n",
    "l2_loss = np.linalg.norm(cds_pred-cds_gt)/np.linalg.norm(cds_gt)\n",
    "print('cds, MAE: %.4f; RMSE: %.4f; R2: %.4f; relative l2: %.4f' %(cds_mae, cds_rmse, cds_r2_score, l2_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# fig = px.scatter(x=pred_1d,\n",
    "#                  y=gts_1d\n",
    "#                 )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# tmp = pred_1d# pred_1d- gts_1d\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=pcd_np[:,0],\n",
    "#     y=pcd_np[:,1],\n",
    "#     z=pcd_np[:,2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=3,\n",
    "#         color=tmp,   # pred[:,0].to('cpu')\n",
    "#         colorscale='Viridis',   # choose a colorscale\n",
    "#         opacity=1\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # tight layout\n",
    "# fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure(data=[go.Scatter3d(\n",
    "#     x=pcd_all_np[:,0],\n",
    "#     y=pcd_all_np[:,1],\n",
    "#     z=pcd_all_np[:,2],\n",
    "#     mode='markers',\n",
    "#     marker=dict(\n",
    "#         size=2,\n",
    "#         color=pred_1d_all,                # set color to an array/list of desired values\n",
    "#         colorscale='Viridis',   # choose a colorscale\n",
    "#         opacity=1\n",
    "#     )\n",
    "# )])\n",
    "\n",
    "# # tight layout\n",
    "# fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics \n",
    "# mae = metrics.mean_absolute_error(pred_1d, gts_1d)\n",
    "# rmse = metrics.mean_squared_error(pred_1d, gts_1d)**0.5\n",
    "# r2_score = metrics.r2_score(pred_1d, gts_1d)\n",
    "# print('Direct, MAE: %.4f; RMSE: %.4f; R2: %.4f ' %(mae, rmse, r2_score))\n",
    "\n",
    "# mae_all = metrics.mean_absolute_error(pred_1d_all, gts_1d_all)\n",
    "# rmse_all = metrics.mean_squared_error(pred_1d_all, gts_1d_all)**0.5\n",
    "# r2_score_all = metrics.r2_score(pred_1d_all, gts_1d_all)\n",
    "# print('Interp, MAE: %.4f; RMSE: %.4f; R2: %.4f ' %(mae_all, rmse_all, r2_score_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save NN to need_frozen_list.csv\n",
    "# path = 'nn_list.txt'\n",
    "# data = np.loadtxt(path, dtype=str, delimiter=' ')\n",
    "# nn_list = np.expand_dims(data,axis=1)\n",
    "# need_frozen = np.ones((len(data),1),dtype=str)\n",
    "# need_frozen_list = np.concatenate((nn_list,need_frozen),axis=1)\n",
    "# np.savetxt('need_frozen_list.csv',need_frozen_list, fmt='%s',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need_frozen_list_path = 'need_frozen_list.csv'\n",
    "# need_frozen_np = np.loadtxt(need_frozen_list_path, dtype=str, delimiter=',')\n",
    "# need_frozen_list = {}\n",
    "\n",
    "# for i, a in enumerate(need_frozen_np[:,0]):\n",
    "#     need_frozen_list[a] = need_frozen_np[i,1]\n",
    "# for param in model.named_parameters():\n",
    "#     if need_frozen_list[param[0]] == '1':\n",
    "#         # frozen\n",
    "#         param[1].requires_grad = False\n",
    "#     else:\n",
    "#         param[1].requires_grad = True\n",
    "#     print(param[0],param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
